# -*- coding: utf-8 -*-
"""Recurrent Neural Network For Text Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zBKS_VO1al_QsERN7taoXMr3_5D1miXs

>RNNs for text


```
Handle sequences for varying length
maintain an internal short-term memory
CNNs spot patterns in chunks
RNNs remember past words for greater meaning

```

Recap: Implementing Dataset and DataLoader
"""

from torch.utils.data import Dataset, DataLoader
class TextDataset(Dataset):
  def __init__(self, text):
    self.text = text
  def __len__(self):
    return len(self.text)
  def __getitem__(self, idx)
    return self.text[idx]

sample_tweet = "This movie had a great plot and amazing acting."
#preprocess the review and convert it to a tensor (not shown for brevity)
sentiment_prediction = model(sample_tweet_tensor)
#train an RNN model to classify tweet as positive or negative

"""What if the tweet is not straight forward?
tweet:


```
"Loved the cinematography
hated the dialogue.
The acting was exceptional
but the plot fell flat "
```
These complex sentences contain subtle nuances and conflicting sentiments. while RNN may struggle to capture the negative sentiment.
Long Short Term Memory models or LSTMs excel at capturing such complexities. They can effectively understand the underlying emotions, making them a powerful tool for sentiment analysis.

LSTM architecture : input gate, forget gate, and output gate
This architecture is idel for complex classification task
"""

class LSTMModel(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(LSTMModel,self).__init__()
    self.LSTM = nn.LSTM(input_size,hidden_size,batch_first = True)
    self.fc = nn.Linear(hidden_size, output_size)

  def forward(self, x):
    _,(hidden,_) = self.lstm(x)
    output = self.fc(hidden.squeeze(0))
    return output

"""```
RNN variation:GRU
Gated Recurrent Unit(GRU) can quickly recognize spammy patterns without
needing the full context

```

GRU
"""

class GRUModel(nn.Module):
  def __init__(self, input_size,hidden_size, output_size):
    super(GRUModel,self).__init__()
    self.gru = nn.GRU(input_size, hidden_size, batch_first = True)
    self.fc = nn.Linear(hidden_size, output_size)

  def forward(self, x):
    _,hidden = self.gru(x)
    output = self.fc(hidden.squeeze(0))
    return output

"""Building RNN model"""

#practice
# Complete the RNN class
class RNNModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNNModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, _ = self.rnn(x, h0)
        out = out[:, -1, :]
        out = self.fc(out)
        return out

# Initialize the model
rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)

# Train the model for ten epochs and zero the gradients
for epoch in range(10):
    optimizer.zero_grad()
    outputs = rnn_model(X_train_seq)
    loss = criterion(outputs, y_train_seq)
    loss.backward()
    optimizer.step()
    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')

# Initialize the LSTM and the output layer with parameters
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, _ = self.lstm(x, (h0, c0))
        out = out[:, -1, :]
        out = self.fc(out)
        return out

# Initialize model with required parameters
lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)

# Train the model by passing the correct parameters and zeroing the gradient
for epoch in range(10):
    optimizer.zero_grad()
    outputs = lstm_model(X_train_seq)
    loss = criterion(outputs, y_train_seq)
    loss.backward()
    optimizer.step()
    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')

# Complete the GRU model
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        out, _ = self.gru(x, h0)
        out = out[:, -1, :]
        out = self.fc(out)
        return out

# Initialize the model
gru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(gru_model.parameters(), lr=0.01)

# Train the model and backpropagate the loss after initialization
for epoch in range(15):
    optimizer.zero_grad()
    outputs = gru_model(X_train_seq)
    loss = criterion(outputs, y_train_seq)
    loss.backward()
    optimizer.step()
    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')

